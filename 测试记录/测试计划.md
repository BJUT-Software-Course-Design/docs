# OnlineJudge 线上代码评测系统

## 测试计划与分析报告

---

## 1. 引言

### 1.1 编写目的

这份测试计划旨在为OnlineJudge线上代码评测系统的测试过程提供指导，明确测试的目标、范围、方法和要求。目标读者为开发团队、测试团队、项目经理以及系统的利益相关者。

### 1.2 项目背景

OnlineJudge系统是一种用于自动化评测编程题目提交情况的系统，主要服务于高校和编程比赛。系统由[委托单位]开发，并由[主管部门]监管。该系统的主要功能包括：接收和管理用户提交的代码、自动化编译和运行、评测代码的正确性和效率、生成评测报告等。

### 1.3 定义

- **OnlineJudge**: 用于在线评测代码的系统。
- **测试用例**: 用于验证系统是否按预期工作的输入、操作和预期输出。
- **Bug**: 软件中的缺陷或错误，导致系统未按预期工作。
- **测试工具**: 用于辅助测试过程的软件或平台，如Jenkins, Selenium等。
- **评测**: 运行用户提交的代码并检查其输出是否符合预期的过程。

### 1.4 参考资料

- 《软件测试技术导论》, 作者: XXX, 出版日期: 2020年, 出版社: XXX出版社
- 《软件工程导论》, 作者: XXX, 出版日期: 2019年, 出版社: XXX出版社
- OnlineJudge系统设计文档，日期: 2024年5月
- OnlineJudge系统用户手册，日期: 2024年6月

---

## 2. 测试计划

### 2.1 测试条件

测试资源需求包括：

- **硬件设备**:
  - 服务器: 配置高效能CPU, 足够的RAM和硬盘空间，运行Linux操作系统。
  - 客户端计算机: 多台运行Windows、Linux和macOS的计算机，以覆盖不同用户环境。
- **软件**:
  - 操作系统: Linux (服务器)，Windows/Linux/macOS (客户端)
  - 数据库: MySQL或PostgreSQL
  - Web服务器: Nginx或Apache
  - 测试工具: JMeter, Selenium, Jenkins
- **人员**:
  - 开发团队: 负责系统开发和bug修复
  - 测试团队: 负责设计和执行测试用例
  - 系统管理员: 负责服务器的设置和维护

### 2.2 测试方案设计

测试方法和用例设计如下：

1. **单元测试**:
   - **目标**: 验证系统中每个独立模块的功能。
   - **工具**: JUnit, PyTest
   - **测试用例**:
     - 检查各模块的函数返回值是否符合预期。
     - 检查输入参数的边界情况（如空值、负值、极大值等）。

2. **集成测试**:
   - **目标**: 验证各模块之间的交互是否正确。
   - **工具**: JUnit, TestNG
   - **测试用例**:
     - 检查用户提交代码后的评测流程。
     - 检查数据库操作的完整性和一致性。

3. **系统测试**:
   - **目标**: 验证系统在整合所有模块后是否按预期工作。
   - **工具**: Selenium, Postman
   - **测试用例**:
     - 用户登录、提交代码、查看评测结果。
     - 管理员管理用户和题目。

4. **性能测试**:
   - **目标**: 测试系统在高负载下的表现。
   - **工具**: JMeter
   - **测试用例**:
     - 模拟大量用户同时提交代码。
     - 测试系统在长时间高负载下的稳定性。

5. **回归测试**:
   - **目标**: 确保在系统修改或升级后，原有功能仍然正常。
   - **工具**: Jenkins, Selenium
   - **测试用例**:
     - 重新运行所有功能测试用例。
     - 检查更新部分的兼容性。

6. **用户接受测试**:
   - **目标**: 验证系统是否满足最终用户的需求和期望。
   - **工具**: 实际用户测试
   - **测试用例**:
     - 让用户进行实际操作，反馈使用体验。
     - 记录用户遇到的所有问题和建议。

---

## 3. 测试执行情况

### 3.1 软硬件环境

- **操作系统**:
  - 服务器: Ubuntu 20.04 LTS
  - 客户端: Windows 10, Ubuntu 20.04, macOS Big Sur
- **测试工具**:
  - 单元测试: JUnit 5, PyTest
  - 集成测试: TestNG, Postman
  - 系统测试: Selenium
  - 性能测试: JMeter
  - 回归测试: Jenkins
- **硬件配置**:
  - 服务器: Intel Xeon E5, 32GB RAM, 1TB SSD
  - 客户端: 各类配置的PC和Mac，模拟不同用户环境。

### 3.2 测试结果

测试结果将记录以下内容：

- **实测结果数据**:
  - 每个测试用例的实际输出。
- **与预期结果数据的偏差**:
  - 对比实际输出和预期输出的差异。
- **该项测试表明的事实**:
  - 测试中发现的系统性能和功能表现。
- **该项测试发现的问题**:
  - 列出所有测试中发现的缺陷和问题。

---

## 4. 评价

### 4.1 范围

本次测试覆盖了OnlineJudge系统的核心功能，包括用户管理、题目管理、代码提交、自动评测和结果反馈。测试的局限性在于，无法完全模拟所有可能的用户行为和环境。

### 4.2 准则

评价测试结果的准则包括：

- **功能性**: 系统是否正确实现所有预期功能。
- **性能**: 系统在高负载下的响应时间和稳定性。
- **兼容性**: 系统在不同操作系统和浏览器下的表现。
- **用户体验**: 系统是否易于使用，用户反馈是否积极。

### 4.3 软件能力

测试结果表明，OnlineJudge系统能够高效地接收、编译和评测用户提交的代码，并能准确反馈结果。系统性能良好，能够处理大量并发请求。

### 4.4 缺陷和限制

- **功能缺陷**: 某些边界条件下的输入可能导致系统异常。
- **性能限制**: 在极端高负载情况下，系统响应时间可能变慢。
- **兼容性问题**: 某些浏览器版本可能会出现样式显示问题。

### 4.5 建议

- **功能改进**: 增加对更多边界条件的处理，确保系统的鲁棒性。
- **性能优化**: 优化数据库查询和缓存机制，以提高系统在高负载下的响应速度。
- **兼容性提升**: 增加对不同浏览器版本的测试，确保系统在所有主流浏览器中的一致性表现。
  